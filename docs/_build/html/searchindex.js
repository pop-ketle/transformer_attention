Search.setIndex({docnames:["attention","common_layer","embedding","index","modules"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":3,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":2,"sphinx.domains.rst":2,"sphinx.domains.std":1,sphinx:56},filenames:["attention.rst","common_layer.rst","embedding.rst","index.rst","modules.rst"],objects:{"":{common_layer:[1,0,0,"-"],embedding:[2,0,0,"-"]},"common_layer.FeedForwardNetwork":{call:[1,2,1,""]},"common_layer.LayerNormalization":{build:[1,2,1,""],call:[1,2,1,""]},"common_layer.ResidualNormalizationWrapper":{call:[1,2,1,""]},common_layer:{FeedForwardNetwork:[1,1,1,""],LayerNormalization:[1,1,1,""],ResidualNormalizationWrapper:[1,1,1,""]},embedding:{TokenEmbedding:[2,1,1,""]}},objnames:{"0":["py","module","Python \u30e2\u30b8\u30e5\u30fc\u30eb"],"1":["py","class","Python \u30af\u30e9\u30b9"],"2":["py","method","Python \u30e1\u30bd\u30c3\u30c9"]},objtypes:{"0":"py:module","1":"py:class","2":"py:method"},terms:{"'s":1,").":1,"*kwargs":1,"--":1,"-between":1,"-creation":1,"-wise":1,".base":[1,2],".engine":[1,2],".float":2,".framework":1,".g":1,".keras":[1,2],".layer":[1,2],".model":1,".ops":1,".python":[1,2],".tensor":1,".training":1,"/tuple":1,"1e":1,"\u3059\u308b":[1,2],"\u306a\u308b":1,"\u306b\u5bfe\u3057":1,"\u3082\u3057\u304f":1,"\u3088\u3046":1,"\u3089\u308c":1,"\u30c8\u30fc\u30af\u30f3":2,"\u30ce\u30fc\u30de\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3":1,"\u30d1\u30e9\u30e1\u30fc\u30bf":1,"\u30d9\u30fc\u30b9\u30af\u30e9\u30b9":[1,2],"\u30da\u30fc\u30b8":3,"\u30e2\u30b8\u30e5\u30fc\u30eb":3,"\u30e2\u30c7\u30eb":1,"\u30ec\u30a4\u30e4\u30fc":1,"\u30ec\u30a4\u30e4\u30fc\u30ce\u30fc\u30de\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3":1,"\u4e0b\u8a18":1,"\u4e0e\u3048":1,"\u504f\u5dee":1,"\u51fa\u529b":1,"\u5909\u63db":2,"\u5e73\u5747":1,"\u623b\u308a\u5024":1,"\u691c\u7d22":3,"\u6a19\u6e96":1,"\u7d22\u5f15":3,"\u884c\u3046":1,"\u8abf\u6574":1,"\u9069\u7528":1,"boolean":1,"case":1,"class":[1,2],"float":1,"for":1,"if":1,"in":1,"int":2,"new":1,"this":1,_dim:[1,2],_layer:[2,3,4],_rate:1,_shape:1,_size:[1,2],additional:1,all:1,and:1,are:1,args:[1,2],arguments:1,attention:4,batch:1,be:1,bias:1,bool:1,build:1,call:1,calls:1,can:1,common:[3,4],computational:1,connection:1,create:1,creates:1,dropout:1,dtype:2,either:1,embedded:2,embedding:[3,4],epsilon:1,expects:1,feed:1,feedforwardnetwork:1,forward:1,from:1,graph:1,hidden:1,implementers:1,indicating:1,inference:1,input:1,inputs:1,instance:1,instances:1,instantiation:1,is:1,just:1,keyword:1,kwargs:[1,2],layer:1,layernormalization:1,length:1,lengthm:1,list:1,lives:1,logic:1,mask:1,masks:1,method:1,mode:1,model:1,module:[3,4],more:1,need:1,network:1,neural:1,no:1,none:1,normalization:1,of:1,on:1,one:1,ops:1,optional:1,or:1,output:1,outputs:1,override:1,overview:3,per:1,position:1,provided:1,reapplies:1,residual:1,residualnormalizationwrapper:1,run:1,scalar:1,scale:1,shape:1,single:1,state:1,step:1,subclass:1,subclasses:1,tensor:1,tensorflow:[1,2],tensors:1,tensorshape:1,tf:[1,2],than:1,that:1,the:1,there:1,they:1,to:1,tokenembedding:2,training:1,transformer:1,typically:1,used:1,variables:1,vector:2,vocab:2,weights:1,where:1,whether:1},titles:["<span class=\"section-number\">1. </span>attention module","<span class=\"section-number\">2. </span>common_layer module","<span class=\"section-number\">3. </span>embedding module","Documentation of Attention Architecture!","transformer_attention"],titleterms:{_attention:4,_layer:1,and:3,architecture:3,attention:[0,3],common:1,documentation:3,embedding:2,indices:3,module:[0,1,2],of:3,tables:3,transformer:4}})